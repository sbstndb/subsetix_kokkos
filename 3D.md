# Étude Approfondie : Améliorations subsetix_kokkos

## Table des matières
1. [Analyse API](#1-analyse-api)
2. [Analyse Host/Device et Synchronisation](#2-analyse-hostdevice-et-synchronisation)
3. [Memory Pool Global](#3-memory-pool-global)
4. [Recommandations Prioritaires](#4-recommandations-prioritaires)

---

## 1. Analyse API

### 1.1 État actuel - Problèmes identifiés

#### A. Verbosité excessive
```cpp
// Code actuel - 5 lignes pour une opération simple
CsrSetAlgebraContext ctx;
IntervalSet2DDevice out = allocate_interval_set_device(cap_rows, cap_intervals);
set_union_device(A, B, out, ctx);
compute_cell_offsets_device(out);
```

#### B. Duplication systématique 2D/3D
Chaque fonction existe en 4+ variantes :
- `set_union_device_dim<2>()` / `set_union_device_dim<3>()`
- `set_union_device()` pour 2D
- `set_union_device()` pour 3D (overload)

**Impact** : ~40% du code est de la duplication boilerplate.

#### C. Manque de composabilité
```cpp
// Impossible de chaîner les opérations
auto result = union(A, B);      // OK
auto final = intersection(result, C);  // Faut gérer result manuellement
```

#### D. Gestion d'erreurs par exceptions
```cpp
throw std::runtime_error("insufficient capacity");  // Crash si non catchée
```

### 1.2 Propositions d'amélioration API

#### A. Pattern Builder Fluent

```cpp
namespace subsetix::csr {

template <int Dim>
class SetOpsBuilder {
public:
  using Geometry = IntervalSetView<Dim, DeviceMemorySpace>;
  
private:
  CsrSetAlgebraContext& ctx_;
  Geometry current_;
  bool owns_current_ = false;
  
public:
  explicit SetOpsBuilder(CsrSetAlgebraContext& ctx) : ctx_(ctx) {}
  
  // Point d'entrée
  SetOpsBuilder& from(const Geometry& input) {
    current_ = input;
    owns_current_ = false;
    return *this;
  }
  
  // Opérations chainables
  SetOpsBuilder& union_with(const Geometry& other) {
    Geometry result = allocate_for_union(current_, other);
    set_union_device_dim<Dim>(current_, other, result, ctx_);
    if (owns_current_) release(current_);
    current_ = result;
    owns_current_ = true;
    return *this;
  }
  
  SetOpsBuilder& intersect_with(const Geometry& other) {
    Geometry result = allocate_for_intersection(current_, other);
    set_intersection_device_dim<Dim>(current_, other, result, ctx_);
    if (owns_current_) release(current_);
    current_ = result;
    owns_current_ = true;
    return *this;
  }
  
  SetOpsBuilder& subtract(const Geometry& other) {
    Geometry result = allocate_for_difference(current_, other);
    set_difference_device_dim<Dim>(current_, other, result, ctx_);
    if (owns_current_) release(current_);
    current_ = result;
    owns_current_ = true;
    return *this;
  }
  
  SetOpsBuilder& expand(Coord rx, Coord ry) {
    Geometry result;
    expand_device_dim<Dim>(current_, rx, ry, result, ctx_);
    if (owns_current_) release(current_);
    current_ = result;
    owns_current_ = true;
    return *this;
  }
  
  SetOpsBuilder& shrink(Coord rx, Coord ry) {
    Geometry result;
    shrink_device_dim<Dim>(current_, rx, ry, result, ctx_);
    if (owns_current_) release(current_);
    current_ = result;
    owns_current_ = true;
    return *this;
  }
  
  // Finalisation
  Geometry build() {
    compute_cell_offsets_device_dim<Dim>(current_);
    owns_current_ = false;
    return current_;
  }
  
  // Estimation de capacité automatique
  Geometry build_into(Geometry& preallocated) {
    // Copie dans buffer préalloué
    return preallocated;
  }
};

// Factory functions
template <int Dim>
SetOpsBuilder<Dim> set_ops(CsrSetAlgebraContext& ctx) {
  return SetOpsBuilder<Dim>(ctx);
}

// Raccourcis pour 2D/3D
inline auto set_ops_2d(CsrSetAlgebraContext& ctx) { return set_ops<2>(ctx); }
inline auto set_ops_3d(CsrSetAlgebraContext& ctx) { return set_ops<3>(ctx); }

} // namespace subsetix::csr
```

**Usage :**
```cpp
CsrSetAlgebraContext ctx;

// Avant (verbeux)
IntervalSet2DDevice tmp1, tmp2, result;
set_union_device(A, B, tmp1, ctx);
set_intersection_device(tmp1, C, tmp2, ctx);
expand_device(tmp2, 2, 2, result, ctx);
compute_cell_offsets_device(result);

// Après (fluent)
auto result = set_ops_2d(ctx)
    .from(A)
    .union_with(B)
    .intersect_with(C)
    .expand(2, 2)
    .build();
```

#### B. Expression Templates pour Lazy Evaluation

```cpp
namespace subsetix::csr::expr {

// Tags pour les opérations
struct UnionTag {};
struct IntersectionTag {};
struct DifferenceTag {};

template <int Dim, typename LHS, typename RHS, typename Op>
struct BinarySetExpr {
  const LHS& lhs;
  const RHS& rhs;
  
  // Évaluation différée
  auto evaluate(CsrSetAlgebraContext& ctx) const {
    auto left = eval_if_expr(lhs, ctx);
    auto right = eval_if_expr(rhs, ctx);
    
    IntervalSetView<Dim, DeviceMemorySpace> result;
    if constexpr (std::is_same_v<Op, UnionTag>) {
      set_union_device_dim<Dim>(left, right, result, ctx);
    } else if constexpr (std::is_same_v<Op, IntersectionTag>) {
      set_intersection_device_dim<Dim>(left, right, result, ctx);
    }
    return result;
  }
};

// Opérateurs surchargés
template <int Dim>
auto operator|(const IntervalSetView<Dim, DeviceMemorySpace>& a,
               const IntervalSetView<Dim, DeviceMemorySpace>& b) {
  return BinarySetExpr<Dim, decltype(a), decltype(b), UnionTag>{a, b};
}

template <int Dim>
auto operator&(const IntervalSetView<Dim, DeviceMemorySpace>& a,
               const IntervalSetView<Dim, DeviceMemorySpace>& b) {
  return BinarySetExpr<Dim, decltype(a), decltype(b), IntersectionTag>{a, b};
}

// Évaluateur
template <int Dim, typename Expr>
auto evaluate(const Expr& expr, CsrSetAlgebraContext& ctx) {
  return expr.evaluate(ctx);
}

} // namespace subsetix::csr::expr
```

**Usage :**
```cpp
using namespace subsetix::csr::expr;

// Syntaxe naturelle
auto result = evaluate((A | B) & C, ctx);  // Union puis Intersection

// Avec estimation automatique des capacités
auto result = evaluate(A | B | C | D, ctx);  // Fusion de 4 ensembles
```

#### C. Result Type avec Error Handling

```cpp
namespace subsetix::csr {

enum class CsrError {
  Ok = 0,
  InsufficientRowCapacity,
  InsufficientIntervalCapacity,
  InvalidInput,
  AllocationFailed,
  CudaError,
};

template <typename T>
struct Result {
  T value;
  CsrError error = CsrError::Ok;
  std::string message;
  
  bool ok() const { return error == CsrError::Ok; }
  operator bool() const { return ok(); }
  
  T& operator*() { return value; }
  const T& operator*() const { return value; }
  
  T* operator->() { return &value; }
  const T* operator->() const { return &value; }
  
  // Chaînage monadique
  template <typename F>
  auto and_then(F&& f) -> Result<decltype(f(value))> {
    if (!ok()) return {decltype(f(value)){}, error, message};
    return f(value);
  }
  
  template <typename F>
  Result& or_else(F&& f) {
    if (!ok()) f(error, message);
    return *this;
  }
};

// Factory
template <typename T>
Result<T> make_ok(T&& value) {
  return {std::forward<T>(value), CsrError::Ok, ""};
}

template <typename T>
Result<T> make_error(CsrError err, const std::string& msg = "") {
  return {T{}, err, msg};
}

// API améliorée
template <int Dim>
Result<IntervalSetView<Dim, DeviceMemorySpace>>
try_set_union(const IntervalSetView<Dim, DeviceMemorySpace>& A,
              const IntervalSetView<Dim, DeviceMemorySpace>& B,
              IntervalSetView<Dim, DeviceMemorySpace>& out,
              CsrSetAlgebraContext& ctx) {
  // Vérifications préalables
  const std::size_t max_rows = A.num_rows + B.num_rows;
  const std::size_t max_intervals = A.num_intervals + B.num_intervals;
  
  if (out.row_keys.extent(0) < max_rows) {
    return make_error<IntervalSetView<Dim, DeviceMemorySpace>>(
        CsrError::InsufficientRowCapacity,
        "Need " + std::to_string(max_rows) + " rows, have " + 
        std::to_string(out.row_keys.extent(0)));
  }
  
  // ... exécution normale ...
  set_union_device_dim<Dim>(A, B, out, ctx);
  
  return make_ok(out);
}

} // namespace subsetix::csr
```

**Usage :**
```cpp
auto result = try_set_union(A, B, out, ctx);
if (!result) {
  std::cerr << "Error: " << result.message << std::endl;
  // Réallocation et retry
}

// Ou avec chaînage
try_set_union(A, B, out, ctx)
    .and_then([&](auto& geom) { return try_expand(geom, 2, 2, ctx); })
    .or_else([](CsrError e, const std::string& msg) {
      std::cerr << "Pipeline failed: " << msg << std::endl;
    });
```

#### D. Geometry Builder avec DSL

```cpp
namespace subsetix::csr {

template <int Dim>
class GeometryBuilder {
  IntervalSetHost<Dim> host_geom_;
  
public:
  // Primitives 2D
  template <int D = Dim, typename = std::enable_if_t<D == 2>>
  GeometryBuilder& add_rect(Coord x0, Coord y0, Coord x1, Coord y1) {
    for (Coord y = y0; y < y1; ++y) {
      RowKey<2> key; key.c[0] = y;
      host_geom_.row_keys.push_back(key);
      host_geom_.row_ptr.push_back(host_geom_.intervals.size());
      host_geom_.intervals.push_back({x0, x1});
    }
    host_geom_.row_ptr.push_back(host_geom_.intervals.size());
    return *this;
  }
  
  template <int D = Dim, typename = std::enable_if_t<D == 2>>
  GeometryBuilder& add_circle(Coord cx, Coord cy, Coord radius) {
    for (Coord y = cy - radius; y <= cy + radius; ++y) {
      Coord dy = y - cy;
      Coord dx = static_cast<Coord>(std::sqrt(radius * radius - dy * dy));
      if (dx > 0) {
        RowKey<2> key; key.c[0] = y;
        host_geom_.row_keys.push_back(key);
        host_geom_.row_ptr.push_back(host_geom_.intervals.size());
        host_geom_.intervals.push_back({cx - dx, cx + dx + 1});
      }
    }
    host_geom_.row_ptr.push_back(host_geom_.intervals.size());
    return *this;
  }
  
  // Primitives 3D
  template <int D = Dim, typename = std::enable_if_t<D == 3>>
  GeometryBuilder& add_box(Coord x0, Coord y0, Coord z0,
                           Coord x1, Coord y1, Coord z1) {
    for (Coord z = z0; z < z1; ++z) {
      for (Coord y = y0; y < y1; ++y) {
        RowKey<3> key; key.c[0] = z; key.c[1] = y;
        host_geom_.row_keys.push_back(key);
        host_geom_.row_ptr.push_back(host_geom_.intervals.size());
        host_geom_.intervals.push_back({x0, x1});
      }
    }
    host_geom_.row_ptr.push_back(host_geom_.intervals.size());
    return *this;
  }
  
  template <int D = Dim, typename = std::enable_if_t<D == 3>>
  GeometryBuilder& add_sphere(Coord cx, Coord cy, Coord cz, Coord radius) {
    for (Coord z = cz - radius; z <= cz + radius; ++z) {
      Coord dz = z - cz;
      Coord rz = static_cast<Coord>(std::sqrt(radius * radius - dz * dz));
      for (Coord y = cy - rz; y <= cy + rz; ++y) {
        Coord dy = y - cy;
        Coord dx = static_cast<Coord>(
            std::sqrt(rz * rz - dy * dy));
        if (dx > 0) {
          RowKey<3> key; key.c[0] = z; key.c[1] = y;
          host_geom_.row_keys.push_back(key);
          host_geom_.row_ptr.push_back(host_geom_.intervals.size());
          host_geom_.intervals.push_back({cx - dx, cx + dx + 1});
        }
      }
    }
    host_geom_.row_ptr.push_back(host_geom_.intervals.size());
    return *this;
  }
  
  // Opérations booléennes côté host (pour setup)
  GeometryBuilder& merge(const GeometryBuilder& other) {
    // Fusion des géométries host
    return *this;
  }
  
  // Build final
  IntervalSetView<Dim, DeviceMemorySpace> build_device() {
    host_geom_.rebuild_mapping();
    return build_device_from_host_dim<Dim>(host_geom_);
  }
  
  IntervalSetHost<Dim> build_host() {
    host_geom_.rebuild_mapping();
    return host_geom_;
  }
};

// Factory
template <int Dim>
GeometryBuilder<Dim> geometry() { return {}; }

inline auto geometry_2d() { return geometry<2>(); }
inline auto geometry_3d() { return geometry<3>(); }

} // namespace subsetix::csr
```

**Usage :**
```cpp
// Construction de géométrie complexe
auto domain = geometry_2d()
    .add_rect(0, 0, 100, 100)      // Domaine principal
    .add_circle(50, 50, 30)        // Zone circulaire
    .build_device();

auto obstacle = geometry_3d()
    .add_box(10, 10, 10, 20, 20, 20)
    .add_sphere(50, 50, 50, 15)
    .build_device();
```

#### E. Field Operations avec CRTP

```cpp
namespace subsetix::csr {

// Mixin pour les opérations sur champs
template <int Dim, typename T, typename Derived>
class FieldOpsMixin {
protected:
  Derived& derived() { return static_cast<Derived&>(*this); }
  const Derived& derived() const { return static_cast<const Derived&>(*this); }
  
public:
  // Fill
  Derived& fill(const T& value) {
    auto& field = derived().field();
    Kokkos::deep_copy(field.values, value);
    return derived();
  }
  
  // Fill on mask
  Derived& fill_on(const IntervalSetView<Dim, DeviceMemorySpace>& mask,
                   const T& value) {
    fill_on_set_device_dim<Dim>(derived().field(), mask, value);
    return derived();
  }
  
  // Scale
  Derived& scale(const T& alpha) {
    auto values = derived().field().values;
    Kokkos::parallel_for(
        Kokkos::RangePolicy<ExecSpace>(0, values.extent(0)),
        KOKKOS_LAMBDA(int i) { values(i) *= alpha; });
    return derived();
  }
  
  // Axpy: y = alpha * x + y
  Derived& axpy(const T& alpha, const Field<Dim, T, DeviceMemorySpace>& x) {
    auto y_vals = derived().field().values;
    auto x_vals = x.values;
    Kokkos::parallel_for(
        Kokkos::RangePolicy<ExecSpace>(0, y_vals.extent(0)),
        KOKKOS_LAMBDA(int i) { y_vals(i) += alpha * x_vals(i); });
    return derived();
  }
  
  // Reduce
  T sum() const {
    T result = 0;
    auto values = derived().field().values;
    Kokkos::parallel_reduce(
        Kokkos::RangePolicy<ExecSpace>(0, values.extent(0)),
        KOKKOS_LAMBDA(int i, T& acc) { acc += values(i); },
        result);
    return result;
  }
  
  T max() const {
    T result = std::numeric_limits<T>::lowest();
    auto values = derived().field().values;
    Kokkos::parallel_reduce(
        Kokkos::RangePolicy<ExecSpace>(0, values.extent(0)),
        KOKKOS_LAMBDA(int i, T& acc) { 
          if (values(i) > acc) acc = values(i); 
        },
        Kokkos::Max<T>(result));
    return result;
  }
  
  T min() const {
    T result = std::numeric_limits<T>::max();
    auto values = derived().field().values;
    Kokkos::parallel_reduce(
        Kokkos::RangePolicy<ExecSpace>(0, values.extent(0)),
        KOKKOS_LAMBDA(int i, T& acc) { 
          if (values(i) < acc) acc = values(i); 
        },
        Kokkos::Min<T>(result));
    return result;
  }
};

// Wrapper avec opérations fluent
template <int Dim, typename T>
class FieldWrapper : public FieldOpsMixin<Dim, T, FieldWrapper<Dim, T>> {
  Field<Dim, T, DeviceMemorySpace> field_;
  
public:
  explicit FieldWrapper(Field<Dim, T, DeviceMemorySpace> f) : field_(std::move(f)) {}
  
  Field<Dim, T, DeviceMemorySpace>& field() { return field_; }
  const Field<Dim, T, DeviceMemorySpace>& field() const { return field_; }
  
  // Conversion implicite
  operator Field<Dim, T, DeviceMemorySpace>&() { return field_; }
};

template <int Dim, typename T>
FieldWrapper<Dim, T> wrap(Field<Dim, T, DeviceMemorySpace>& f) {
  return FieldWrapper<Dim, T>(f);
}

} // namespace subsetix::csr
```

**Usage :**
```cpp
auto field = make_field_like_device(geometry, 0.0);

// Opérations fluent
wrap(field)
    .fill(1.0)
    .fill_on(boundary_mask, 0.0)
    .scale(2.0);

// Réductions
double total = wrap(field).sum();
double max_val = wrap(field).max();
```

---

## 2. Analyse Host/Device et Synchronisation

### 2.1 État actuel - Problèmes critiques

#### A. Synchronisations excessives

**Données collectées :**
- **92 occurrences** de `ExecSpace().fence()` dans les headers
- **45 occurrences** de `Kokkos::deep_copy` (synchrones)

**Exemples problématiques :**

```cpp
// core.hpp - set_union_device_dim
Kokkos::parallel_for("kernel_1", ...);
ExecSpace().fence();  // Sync 1

Kokkos::parallel_for("kernel_2", ...);
ExecSpace().fence();  // Sync 2

Kokkos::parallel_scan("kernel_3", ...);
ExecSpace().fence();  // Sync 3

Kokkos::deep_copy(num_b_only, d_num_b_only);  // Sync 4 (implicite)

// ... etc. Jusqu'à 8+ syncs par opération!
```

**Impact performance :**
- Chaque `fence()` = ~5-20μs de latence GPU
- Deep_copy scalaire = fence + transfert PCIe
- Une opération `set_union` : 6-8 synchronisations = **40-160μs de latence pure**
- Pour des petites géométries, la latence domine le temps de calcul

#### B. Transferts scalaires fréquents

```cpp
// Pattern récurrent : récupération de compteur
Kokkos::View<std::size_t, DeviceMemorySpace> d_count("count");
// ... kernel qui écrit dans d_count ...
std::size_t count;
Kokkos::deep_copy(count, d_count);  // SYNC + TRANSFER!
```

**Problème** : Le CPU attend le GPU pour un simple entier.

#### C. Pas de gestion d'execution space instances

```cpp
// Code actuel - Tout sur le stream par défaut
Kokkos::parallel_for("kernel", Kokkos::RangePolicy<ExecSpace>(0, n), ...);

// Ce qui pourrait être fait
ExecSpace exec;
Kokkos::parallel_for("kernel", Kokkos::RangePolicy<ExecSpace>(exec, 0, n), ...);
// Permet des opérations concurrentes sur différents streams
```

### 2.2 Solutions proposées

#### A. Execution Policy avec Stream Management

```cpp
namespace subsetix::csr {

// Wrapper pour la gestion des streams
class ExecutionContext {
public:
  using exec_space = ExecSpace;
  
private:
  exec_space exec_;
  bool owns_stream_ = false;
  
#ifdef KOKKOS_ENABLE_CUDA
  cudaStream_t cuda_stream_ = 0;
#endif

public:
  ExecutionContext() : exec_() {}
  
#ifdef KOKKOS_ENABLE_CUDA
  // Création d'un stream dédié
  static ExecutionContext create_async() {
    ExecutionContext ctx;
    cudaStreamCreate(&ctx.cuda_stream_);
    ctx.exec_ = exec_space(ctx.cuda_stream_);
    ctx.owns_stream_ = true;
    return ctx;
  }
#endif
  
  ~ExecutionContext() {
#ifdef KOKKOS_ENABLE_CUDA
    if (owns_stream_ && cuda_stream_) {
      cudaStreamDestroy(cuda_stream_);
    }
#endif
  }
  
  exec_space& space() { return exec_; }
  const exec_space& space() const { return exec_; }
  
  void fence() const { exec_.fence(); }
  
  // Synchronisation conditionnelle
  void fence_if_needed(bool needed) const {
    if (needed) exec_.fence();
  }
};

// Contexte de calcul amélioré
struct CsrComputeContext {
  CsrSetAlgebraContext algebra;
  ExecutionContext exec;
  
  // Options de synchronisation
  bool auto_sync = true;  // Sync après chaque op si true
  bool defer_offsets = false;  // Ne pas calculer cell_offsets automatiquement
};

} // namespace subsetix::csr
```

#### B. Async Operations avec Futures

```cpp
namespace subsetix::csr {

// Future pour résultat asynchrone
template <typename T>
class AsyncResult {
  T result_;
  Kokkos::View<int, DeviceMemorySpace> ready_flag_;
  ExecutionContext* exec_;
  
public:
  // Attente explicite
  T& get() {
    exec_->fence();
    return result_;
  }
  
  // Test non-bloquant (CUDA only)
  bool is_ready() const {
#ifdef KOKKOS_ENABLE_CUDA
    return cudaStreamQuery(exec_->cuda_stream()) == cudaSuccess;
#else
    return true;
#endif
  }
  
  // Accès sans attente (UB si pas prêt!)
  T& unsafe_get() { return result_; }
};

// API asynchrone
template <int Dim>
AsyncResult<IntervalSetView<Dim, DeviceMemorySpace>>
set_union_async(const IntervalSetView<Dim, DeviceMemorySpace>& A,
                const IntervalSetView<Dim, DeviceMemorySpace>& B,
                CsrComputeContext& ctx) {
  AsyncResult<IntervalSetView<Dim, DeviceMemorySpace>> result;
  // ... lancement sans fence final ...
  return result;
}

} // namespace subsetix::csr
```

#### C. Batch Operations pour amortir les latences

```cpp
namespace subsetix::csr {

template <int Dim>
class BatchSetOps {
  CsrComputeContext& ctx_;
  std::vector<std::function<void()>> pending_ops_;
  
public:
  explicit BatchSetOps(CsrComputeContext& ctx) : ctx_(ctx) {}
  
  // Enqueue sans exécution
  BatchSetOps& enqueue_union(
      const IntervalSetView<Dim, DeviceMemorySpace>& A,
      const IntervalSetView<Dim, DeviceMemorySpace>& B,
      IntervalSetView<Dim, DeviceMemorySpace>& out) {
    pending_ops_.push_back([&, A, B]() {
      set_union_device_dim_no_sync<Dim>(A, B, out, ctx_);
    });
    return *this;
  }
  
  BatchSetOps& enqueue_intersection(/* ... */) { /* ... */ return *this; }
  
  // Exécution de toutes les ops avec une seule sync finale
  void execute() {
    for (auto& op : pending_ops_) {
      op();
    }
    ctx_.exec.fence();  // Une seule sync!
    pending_ops_.clear();
  }
};

// Variante avec graphes CUDA
#ifdef KOKKOS_ENABLE_CUDA
template <int Dim>
class GraphSetOps {
  cudaGraph_t graph_;
  cudaGraphExec_t graph_exec_;
  bool captured_ = false;
  
public:
  // Capture d'un graphe de calcul
  void begin_capture(cudaStream_t stream) {
    cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal);
  }
  
  void end_capture(cudaStream_t stream) {
    cudaStreamEndCapture(stream, &graph_);
    cudaGraphInstantiate(&graph_exec_, graph_, nullptr, nullptr, 0);
    captured_ = true;
  }
  
  // Replay du graphe (très rapide)
  void replay(cudaStream_t stream) {
    if (captured_) {
      cudaGraphLaunch(graph_exec_, stream);
    }
  }
};
#endif

} // namespace subsetix::csr
```

#### D. Réduction des transferts scalaires

```cpp
namespace subsetix::csr::detail {

// Pool de scalaires device pour éviter les allocations répétées
struct ScalarPool {
  static constexpr int POOL_SIZE = 16;
  
  Kokkos::View<std::size_t[POOL_SIZE], DeviceMemorySpace> size_t_pool;
  Kokkos::View<int[POOL_SIZE], DeviceMemorySpace> int_pool;
  int next_size_t_ = 0;
  int next_int_ = 0;
  
  ScalarPool() 
    : size_t_pool("scalar_pool_size_t"),
      int_pool("scalar_pool_int") {}
  
  // Retourne un sous-view pour un scalaire
  auto get_size_t() {
    int idx = next_size_t_++ % POOL_SIZE;
    return Kokkos::subview(size_t_pool, idx);
  }
  
  auto get_int() {
    int idx = next_int_++ % POOL_SIZE;
    return Kokkos::subview(int_pool, idx);
  }
  
  // Lecture groupée de tous les scalaires utilisés
  void read_all(std::vector<std::size_t>& sizes, std::vector<int>& ints) {
    auto h_sizes = Kokkos::create_mirror_view_and_copy(
        HostMemorySpace{}, size_t_pool);
    auto h_ints = Kokkos::create_mirror_view_and_copy(
        HostMemorySpace{}, int_pool);
    
    sizes.resize(next_size_t_);
    ints.resize(next_int_);
    
    for (int i = 0; i < next_size_t_; ++i) sizes[i] = h_sizes(i % POOL_SIZE);
    for (int i = 0; i < next_int_; ++i) ints[i] = h_ints(i % POOL_SIZE);
    
    next_size_t_ = 0;
    next_int_ = 0;
  }
};

// Pattern "write-once, read-later"
template <int Dim>
struct DeferredSetUnionResult {
  IntervalSetView<Dim, DeviceMemorySpace> geometry;
  Kokkos::View<std::size_t, DeviceMemorySpace> d_num_intervals;
  
  // Lecture différée (une seule sync pour plusieurs résultats)
  std::size_t finalize() {
    std::size_t result;
    Kokkos::deep_copy(result, d_num_intervals);
    geometry.num_intervals = result;
    return result;
  }
};

} // namespace subsetix::csr::detail
```

#### E. Kernel Fusion

```cpp
namespace subsetix::csr {

// Fusion de count + scan + fill en deux passes au lieu de trois
template <int Dim>
inline void set_union_fused(
    const IntervalSetView<Dim, DeviceMemorySpace>& A,
    const IntervalSetView<Dim, DeviceMemorySpace>& B,
    IntervalSetView<Dim, DeviceMemorySpace>& out,
    CsrComputeContext& ctx) {
  
  // Pass 1: Count + Exclusive Scan (fusionné)
  // Utilise parallel_scan avec counting intégré
  auto row_ptr_out = out.row_ptr;
  
  Kokkos::parallel_scan(
      "fused_count_scan",
      Kokkos::RangePolicy<ExecSpace>(ctx.exec.space(), 0, num_rows),
      KOKKOS_LAMBDA(const int i, std::size_t& update, const bool final) {
        // Calcul du count inline
        std::size_t count = compute_row_union_count(i, A, B);
        if (final) {
          row_ptr_out(i) = update;
        }
        update += count;
      });
  
  // Pass 2: Fill (pas de fence entre scan et fill si même stream)
  Kokkos::parallel_for(
      "fill",
      Kokkos::RangePolicy<ExecSpace>(ctx.exec.space(), 0, num_rows),
      KOKKOS_LAMBDA(const int i) {
        fill_row_union(i, A, B, out);
      });
  
  // Une seule fence à la fin
  if (ctx.auto_sync) ctx.exec.fence();
}

} // namespace subsetix::csr
```

#### F. Prefetching et Memory Hints

```cpp
namespace subsetix::csr::detail {

// Prefetch des données CSR avant accès
template <int Dim>
inline void prefetch_interval_set(
    const IntervalSetView<Dim, DeviceMemorySpace>& set,
    cudaStream_t stream = 0) {
#ifdef KOKKOS_ENABLE_CUDA
  cudaMemPrefetchAsync(set.row_keys.data(), 
                       set.row_keys.extent(0) * sizeof(RowKey<Dim>),
                       0, stream);  // 0 = current device
  cudaMemPrefetchAsync(set.row_ptr.data(),
                       set.row_ptr.extent(0) * sizeof(std::size_t),
                       0, stream);
  cudaMemPrefetchAsync(set.intervals.data(),
                       set.intervals.extent(0) * sizeof(Interval),
                       0, stream);
#endif
}

// Advise pour les patterns d'accès
template <int Dim>
inline void advise_read_mostly(
    const IntervalSetView<Dim, DeviceMemorySpace>& set) {
#ifdef KOKKOS_ENABLE_CUDA
  cudaMemAdvise(set.row_keys.data(),
                set.row_keys.extent(0) * sizeof(RowKey<Dim>),
                cudaMemAdviseSetReadMostly, 0);
  // ... same for other views ...
#endif
}

} // namespace subsetix::csr::detail
```

---

## 3. Memory Pool Global

### 3.1 État actuel - Problèmes

#### A. Allocations répétées dans UnifiedCsrWorkspace

```cpp
// Chaque appel à get_int_buf_X peut réallouer
Kokkos::View<int*, DeviceMemorySpace> get_int_buf_0(std::size_t size) {
  ensure_view_capacity(int_buf_0, size, "unified_ws_int_0");
  return int_buf_0;
}

// ensure_view_capacity réalloue si trop petit
template <class ViewType>
inline void ensure_view_capacity(ViewType& view, std::size_t required_size, ...) {
  if (view.extent(0) < required_size) {
    view = ViewType(label, required_size);  // ALLOCATION!
  }
}
```

**Problèmes :**
1. Pas de facteur de croissance → réallocations fréquentes
2. Pas de shrink → mémoire jamais libérée
3. Fragmentation du heap GPU
4. Latence d'allocation CUDA (~100μs-1ms)

#### B. Pas de partage entre contextes

```cpp
// Chaque CsrSetAlgebraContext a son propre workspace
struct CsrSetAlgebraContext {
  detail::UnifiedCsrWorkspace workspace;  // Mémoire dupliquée!
};

// Usage typique avec plusieurs contextes
CsrSetAlgebraContext ctx1, ctx2, ctx3;  // 3x la mémoire!
```

#### C. Allocations cachées dans les opérations

```cpp
// morphology.hpp - allocations dans les kernels
if (out.row_keys.extent(0) < num_rows_out) {
  out.row_keys = IntervalSet2DDevice::RowKeyView("...", num_rows_out);
}
if (out.row_ptr.extent(0) < num_rows_out + 1) {
  out.row_ptr = IntervalSet2DDevice::IndexView("...", num_rows_out + 1);
}
// ... etc.
```

### 3.2 Solution : Memory Pool Hiérarchique

#### A. Architecture globale

```cpp
namespace subsetix::csr {

// Niveau 1: Pool de blocs de taille fixe (slab allocator)
// Niveau 2: Pool de blocs de taille variable (buddy allocator)
// Niveau 3: Fallback vers cudaMalloc

class CsrMemoryPool {
public:
  // Singleton thread-safe
  static CsrMemoryPool& instance() {
    static CsrMemoryPool pool;
    return pool;
  }
  
  struct Config {
    std::size_t initial_pool_size = 256 * 1024 * 1024;  // 256 MB
    std::size_t max_pool_size = 2ULL * 1024 * 1024 * 1024;  // 2 GB
    double growth_factor = 1.5;
    std::size_t alignment = 256;  // CUDA optimal
    bool enable_statistics = true;
  };
  
  struct Statistics {
    std::size_t total_allocated = 0;
    std::size_t peak_usage = 0;
    std::size_t current_usage = 0;
    std::size_t allocation_count = 0;
    std::size_t reuse_count = 0;
    std::size_t fragmentation_ratio = 0;  // En pourcentage
  };
  
private:
  Config config_;
  Statistics stats_;
  
  // Slab allocator pour petites allocations (< 1MB)
  struct SlabAllocator {
    static constexpr std::size_t SLAB_SIZES[] = {
      256, 512, 1024, 2048, 4096, 8192, 16384, 32768,
      65536, 131072, 262144, 524288, 1048576
    };
    static constexpr int NUM_SLABS = 13;
    
    struct Slab {
      void* base = nullptr;
      std::size_t block_size = 0;
      std::size_t num_blocks = 0;
      std::vector<bool> free_bitmap;
      std::size_t free_count = 0;
    };
    
    std::array<std::vector<Slab>, NUM_SLABS> slabs_;
    std::mutex mutex_;
    
    int size_to_slab_index(std::size_t size) const {
      for (int i = 0; i < NUM_SLABS; ++i) {
        if (size <= SLAB_SIZES[i]) return i;
      }
      return -1;
    }
    
    void* allocate(std::size_t size) {
      int idx = size_to_slab_index(size);
      if (idx < 0) return nullptr;
      
      std::lock_guard<std::mutex> lock(mutex_);
      
      // Chercher un bloc libre
      for (auto& slab : slabs_[idx]) {
        if (slab.free_count > 0) {
          for (std::size_t i = 0; i < slab.num_blocks; ++i) {
            if (slab.free_bitmap[i]) {
              slab.free_bitmap[i] = false;
              slab.free_count--;
              return static_cast<char*>(slab.base) + i * slab.block_size;
            }
          }
        }
      }
      
      // Créer nouvelle slab
      return allocate_new_slab(idx);
    }
    
    void deallocate(void* ptr, std::size_t size) {
      int idx = size_to_slab_index(size);
      if (idx < 0) return;
      
      std::lock_guard<std::mutex> lock(mutex_);
      
      for (auto& slab : slabs_[idx]) {
        char* base = static_cast<char*>(slab.base);
        char* p = static_cast<char*>(ptr);
        if (p >= base && p < base + slab.num_blocks * slab.block_size) {
          std::size_t block_idx = (p - base) / slab.block_size;
          slab.free_bitmap[block_idx] = true;
          slab.free_count++;
          return;
        }
      }
    }
    
    void* allocate_new_slab(int idx);  // Impl details
  };
  
  // Buddy allocator pour grandes allocations (1MB - 256MB)
  struct BuddyAllocator {
    static constexpr std::size_t MIN_BLOCK = 1024 * 1024;  // 1 MB
    static constexpr std::size_t MAX_BLOCK = 256 * 1024 * 1024;  // 256 MB
    static constexpr int NUM_LEVELS = 8;  // log2(256/1) = 8
    
    struct Block {
      void* ptr = nullptr;
      std::size_t size = 0;
      bool free = true;
      Block* buddy = nullptr;
      Block* parent = nullptr;
      Block* left = nullptr;
      Block* right = nullptr;
    };
    
    std::vector<Block*> free_lists_[NUM_LEVELS];
    void* pool_base_ = nullptr;
    std::size_t pool_size_ = 0;
    std::mutex mutex_;
    
    void* allocate(std::size_t size) {
      std::size_t aligned_size = next_power_of_2(std::max(size, MIN_BLOCK));
      int level = size_to_level(aligned_size);
      
      std::lock_guard<std::mutex> lock(mutex_);
      return find_or_split(level);
    }
    
    void deallocate(void* ptr) {
      std::lock_guard<std::mutex> lock(mutex_);
      // Trouver le bloc et potentiellement fusionner avec buddy
      coalesce(find_block(ptr));
    }
    
    int size_to_level(std::size_t size) const {
      // Level 0 = MAX_BLOCK, Level NUM_LEVELS-1 = MIN_BLOCK
      return NUM_LEVELS - 1 - static_cast<int>(std::log2(size / MIN_BLOCK));
    }
    
    void* find_or_split(int level);  // Impl
    void coalesce(Block* block);  // Impl
    Block* find_block(void* ptr);  // Impl
  };
  
  SlabAllocator slab_allocator_;
  BuddyAllocator buddy_allocator_;
  
public:
  CsrMemoryPool() = default;
  
  void initialize(const Config& config = {}) {
    config_ = config;
    // Préallouer le pool principal
#ifdef KOKKOS_ENABLE_CUDA
    cudaMalloc(&buddy_allocator_.pool_base_, config_.initial_pool_size);
#else
    buddy_allocator_.pool_base_ = std::malloc(config_.initial_pool_size);
#endif
    buddy_allocator_.pool_size_ = config_.initial_pool_size;
  }
  
  void* allocate(std::size_t size, std::size_t alignment = 0) {
    if (alignment == 0) alignment = config_.alignment;
    std::size_t aligned_size = (size + alignment - 1) & ~(alignment - 1);
    
    stats_.allocation_count++;
    
    void* ptr = nullptr;
    
    // Stratégie: slab < 1MB, buddy 1MB-256MB, cuda > 256MB
    if (aligned_size <= 1024 * 1024) {
      ptr = slab_allocator_.allocate(aligned_size);
    } else if (aligned_size <= 256 * 1024 * 1024) {
      ptr = buddy_allocator_.allocate(aligned_size);
    } else {
      // Fallback
#ifdef KOKKOS_ENABLE_CUDA
      cudaMalloc(&ptr, aligned_size);
#else
      ptr = std::aligned_alloc(alignment, aligned_size);
#endif
    }
    
    if (ptr) {
      stats_.total_allocated += aligned_size;
      stats_.current_usage += aligned_size;
      stats_.peak_usage = std::max(stats_.peak_usage, stats_.current_usage);
    }
    
    return ptr;
  }
  
  void deallocate(void* ptr, std::size_t size) {
    if (!ptr) return;
    
    std::size_t aligned_size = (size + config_.alignment - 1) & 
                               ~(config_.alignment - 1);
    
    if (aligned_size <= 1024 * 1024) {
      slab_allocator_.deallocate(ptr, aligned_size);
    } else if (aligned_size <= 256 * 1024 * 1024) {
      buddy_allocator_.deallocate(ptr);
    } else {
#ifdef KOKKOS_ENABLE_CUDA
      cudaFree(ptr);
#else
      std::free(ptr);
#endif
    }
    
    stats_.current_usage -= aligned_size;
  }
  
  // Allocation typée
  template <typename T>
  T* allocate_array(std::size_t count) {
    return static_cast<T*>(allocate(count * sizeof(T), alignof(T)));
  }
  
  template <typename T>
  void deallocate_array(T* ptr, std::size_t count) {
    deallocate(ptr, count * sizeof(T));
  }
  
  // Création de Kokkos::View depuis le pool
  template <typename T, class MemorySpace = DeviceMemorySpace>
  Kokkos::View<T*, MemorySpace, Kokkos::MemoryTraits<Kokkos::Unmanaged>>
  make_view(std::size_t size, const std::string& label = "pooled_view") {
    T* ptr = allocate_array<T>(size);
    return Kokkos::View<T*, MemorySpace, Kokkos::MemoryTraits<Kokkos::Unmanaged>>(
        ptr, size);
  }
  
  // Garbage collection
  void gc(double target_usage_ratio = 0.5) {
    // Libérer les slabs/blocs inutilisés
    // ...
  }
  
  // Statistiques
  const Statistics& statistics() const { return stats_; }
  
  void print_statistics() const {
    std::cout << "=== CsrMemoryPool Statistics ===" << std::endl;
    std::cout << "Total allocated: " << stats_.total_allocated / (1024*1024) << " MB" << std::endl;
    std::cout << "Peak usage: " << stats_.peak_usage / (1024*1024) << " MB" << std::endl;
    std::cout << "Current usage: " << stats_.current_usage / (1024*1024) << " MB" << std::endl;
    std::cout << "Allocations: " << stats_.allocation_count << std::endl;
    std::cout << "Reuses: " << stats_.reuse_count << std::endl;
  }
  
  // Reset complet
  void reset() {
    // Libérer tout le pool et réinitialiser
  }
};

} // namespace subsetix::csr
```

#### B. Pooled Views avec RAII

```cpp
namespace subsetix::csr {

// RAII wrapper pour views depuis le pool
template <typename T, class MemorySpace = DeviceMemorySpace>
class PooledView {
  using ViewType = Kokkos::View<T*, MemorySpace, Kokkos::MemoryTraits<Kokkos::Unmanaged>>;
  
  ViewType view_;
  std::size_t size_;
  CsrMemoryPool* pool_;
  
public:
  PooledView() : pool_(nullptr), size_(0) {}
  
  PooledView(CsrMemoryPool& pool, std::size_t size, const std::string& label = "")
      : pool_(&pool), size_(size) {
    T* ptr = pool_->allocate_array<T>(size);
    view_ = ViewType(ptr, size);
  }
  
  ~PooledView() {
    if (pool_ && view_.data()) {
      pool_->deallocate_array(view_.data(), size_);
    }
  }
  
  // Move only
  PooledView(PooledView&& other) noexcept
      : view_(other.view_), size_(other.size_), pool_(other.pool_) {
    other.pool_ = nullptr;
    other.view_ = ViewType();
  }
  
  PooledView& operator=(PooledView&& other) noexcept {
    if (this != &other) {
      if (pool_ && view_.data()) {
        pool_->deallocate_array(view_.data(), size_);
      }
      view_ = other.view_;
      size_ = other.size_;
      pool_ = other.pool_;
      other.pool_ = nullptr;
      other.view_ = ViewType();
    }
    return *this;
  }
  
  // Pas de copie
  PooledView(const PooledView&) = delete;
  PooledView& operator=(const PooledView&) = delete;
  
  ViewType& get() { return view_; }
  const ViewType& get() const { return view_; }
  
  T* data() { return view_.data(); }
  std::size_t size() const { return size_; }
  std::size_t extent(int i) const { return view_.extent(i); }
  
  // Conversion implicite
  operator ViewType&() { return view_; }
  operator const ViewType&() const { return view_; }
};

// Factory
template <typename T, class MemorySpace = DeviceMemorySpace>
PooledView<T, MemorySpace> make_pooled_view(
    std::size_t size, 
    const std::string& label = "") {
  return PooledView<T, MemorySpace>(CsrMemoryPool::instance(), size, label);
}

} // namespace subsetix::csr
```

#### C. Workspace amélioré avec pool

```cpp
namespace subsetix::csr::detail {

class PooledWorkspace {
  CsrMemoryPool& pool_;
  
  // Buffers avec tracking de capacité et usage
  struct BufferInfo {
    void* ptr = nullptr;
    std::size_t capacity = 0;
    std::size_t used = 0;
    bool in_use = false;
  };
  
  std::vector<BufferInfo> int_buffers_;
  std::vector<BufferInfo> size_t_buffers_;
  std::vector<BufferInfo> row_key_buffers_;
  std::vector<BufferInfo> interval_buffers_;
  std::vector<BufferInfo> generic_buffers_;
  
public:
  explicit PooledWorkspace(CsrMemoryPool& pool = CsrMemoryPool::instance())
      : pool_(pool) {
    // Préallouer quelques buffers de base
    int_buffers_.resize(8);
    size_t_buffers_.resize(4);
    row_key_buffers_.resize(4);
    interval_buffers_.resize(2);
    generic_buffers_.resize(4);
  }
  
  ~PooledWorkspace() {
    clear();
  }
  
  // Checkout avec facteur de croissance
  Kokkos::View<int*, DeviceMemorySpace, Kokkos::MemoryTraits<Kokkos::Unmanaged>>
  checkout_int(std::size_t size, int buffer_id = 0) {
    if (buffer_id >= static_cast<int>(int_buffers_.size())) {
      int_buffers_.resize(buffer_id + 1);
    }
    
    auto& info = int_buffers_[buffer_id];
    
    if (info.capacity < size) {
      // Libérer l'ancien
      if (info.ptr) {
        pool_.deallocate_array(static_cast<int*>(info.ptr), info.capacity);
      }
      // Allouer avec facteur de croissance
      std::size_t new_capacity = std::max(size, 
          static_cast<std::size_t>(info.capacity * 1.5));
      new_capacity = std::max(new_capacity, std::size_t(1024));  // Minimum
      
      info.ptr = pool_.allocate_array<int>(new_capacity);
      info.capacity = new_capacity;
    }
    
    info.used = size;
    info.in_use = true;
    
    return Kokkos::View<int*, DeviceMemorySpace, 
                        Kokkos::MemoryTraits<Kokkos::Unmanaged>>(
        static_cast<int*>(info.ptr), size);
  }
  
  // Idem pour autres types...
  template <typename T>
  Kokkos::View<T*, DeviceMemorySpace, Kokkos::MemoryTraits<Kokkos::Unmanaged>>
  checkout_generic(std::size_t size, int buffer_id = 0) {
    if (buffer_id >= static_cast<int>(generic_buffers_.size())) {
      generic_buffers_.resize(buffer_id + 1);
    }
    
    auto& info = generic_buffers_[buffer_id];
    std::size_t bytes_needed = size * sizeof(T);
    
    if (info.capacity < bytes_needed) {
      if (info.ptr) {
        pool_.deallocate(info.ptr, info.capacity);
      }
      std::size_t new_capacity = std::max(bytes_needed,
          static_cast<std::size_t>(info.capacity * 1.5));
      new_capacity = std::max(new_capacity, std::size_t(4096));
      
      info.ptr = pool_.allocate(new_capacity);
      info.capacity = new_capacity;
    }
    
    info.used = bytes_needed;
    info.in_use = true;
    
    return Kokkos::View<T*, DeviceMemorySpace,
                        Kokkos::MemoryTraits<Kokkos::Unmanaged>>(
        static_cast<T*>(info.ptr), size);
  }
  
  // Release tous les buffers (marque comme disponible, ne libère pas)
  void release_all() {
    for (auto& info : int_buffers_) info.in_use = false;
    for (auto& info : size_t_buffers_) info.in_use = false;
    for (auto& info : row_key_buffers_) info.in_use = false;
    for (auto& info : interval_buffers_) info.in_use = false;
    for (auto& info : generic_buffers_) info.in_use = false;
  }
  
  // Libération effective de la mémoire
  void clear() {
    for (auto& info : int_buffers_) {
      if (info.ptr) pool_.deallocate_array(static_cast<int*>(info.ptr), info.capacity);
      info = {};
    }
    // ... idem pour autres buffers ...
  }
  
  // Shrink: réduire les buffers surdimensionnés
  void shrink_to_fit(double max_usage_ratio = 0.5) {
    for (auto& info : int_buffers_) {
      if (info.ptr && info.used < info.capacity * max_usage_ratio) {
        pool_.deallocate_array(static_cast<int*>(info.ptr), info.capacity);
        if (info.used > 0) {
          info.ptr = pool_.allocate_array<int>(info.used);
          info.capacity = info.used;
        } else {
          info = {};
        }
      }
    }
    // ... idem pour autres ...
  }
  
  // Statistiques
  std::size_t total_capacity() const {
    std::size_t total = 0;
    for (const auto& info : int_buffers_) total += info.capacity * sizeof(int);
    for (const auto& info : size_t_buffers_) total += info.capacity * sizeof(std::size_t);
    // ...
    return total;
  }
};

} // namespace subsetix::csr::detail
```

#### D. Allocateur Kokkos personnalisé

```cpp
namespace subsetix::csr {

// Allocateur compatible Kokkos
class PooledMemorySpace {
public:
  using memory_space = PooledMemorySpace;
  using execution_space = ExecSpace;
  using device_type = Kokkos::Device<execution_space, memory_space>;
  using size_type = std::size_t;
  
  static constexpr const char* name() { return "CsrPooledMemory"; }
  
  void* allocate(const std::size_t size) const {
    return CsrMemoryPool::instance().allocate(size);
  }
  
  void deallocate(void* ptr, const std::size_t size) const {
    CsrMemoryPool::instance().deallocate(ptr, size);
  }
};

// Usage avec Kokkos::View
template <typename T>
using PooledDeviceView = Kokkos::View<T*, PooledMemorySpace>;

// Création facilitée
template <typename T>
PooledDeviceView<T> make_pooled_device_view(
    std::size_t size, 
    const std::string& label = "pooled") {
  return PooledDeviceView<T>(label, size);
}

} // namespace subsetix::csr
```

#### E. Gestion du cycle de vie

```cpp
namespace subsetix::csr {

// Scope guard pour le workspace
class WorkspaceScope {
  PooledWorkspace& ws_;
  
public:
  explicit WorkspaceScope(PooledWorkspace& ws) : ws_(ws) {}
  ~WorkspaceScope() { ws_.release_all(); }
  
  // Non copyable
  WorkspaceScope(const WorkspaceScope&) = delete;
  WorkspaceScope& operator=(const WorkspaceScope&) = delete;
};

// Contexte complet avec gestion mémoire
class ComputeSession {
  CsrMemoryPool pool_;
  detail::PooledWorkspace workspace_;
  ExecutionContext exec_;
  
public:
  ComputeSession() : workspace_(pool_) {
    pool_.initialize();
  }
  
  ~ComputeSession() {
    workspace_.clear();
    pool_.reset();
  }
  
  // Factory pour opérations
  template <int Dim>
  SetOpsBuilder<Dim> set_ops() {
    return SetOpsBuilder<Dim>(*this);
  }
  
  // Accès aux composants
  CsrMemoryPool& pool() { return pool_; }
  detail::PooledWorkspace& workspace() { return workspace_; }
  ExecutionContext& exec() { return exec_; }
  
  // Scope automatique
  WorkspaceScope scoped_workspace() {
    return WorkspaceScope(workspace_);
  }
};

// Usage recommandé
void example_usage() {
  ComputeSession session;
  
  {
    auto scope = session.scoped_workspace();
    
    auto result = session.set_ops<2>()
        .from(A)
        .union_with(B)
        .build();
        
  }  // workspace automatiquement released
  
  session.pool().print_statistics();
}

} // namespace subsetix::csr
```

---

## 4. Recommandations Prioritaires

### 4.1 Court terme (1-2 semaines)

| Priorité | Action | Impact | Effort |
|----------|--------|--------|--------|
| **P0** | Réduire les `fence()` : grouper 3-4 kernels entre chaque sync | -50% latence | Moyen |
| **P0** | Facteur de croissance 1.5x dans `ensure_view_capacity` | -80% réallocs | Faible |
| **P1** | Ajouter `Result<T>` pour error handling | Robustesse | Faible |
| **P1** | Créer `SetOpsBuilder` fluent | UX dev | Moyen |

### 4.2 Moyen terme (1-2 mois)

| Priorité | Action | Impact | Effort |
|----------|--------|--------|--------|
| **P1** | Implémenter `CsrMemoryPool` basique (slab allocator) | -70% alloc time | Élevé |
| **P1** | `ExecutionContext` avec stream management | Concurrence | Moyen |
| **P2** | Batch operations avec une seule sync | -80% latence batch | Moyen |
| **P2** | `GeometryBuilder` DSL | UX setup | Moyen |

### 4.3 Long terme (3-6 mois)

| Priorité | Action | Impact | Effort |
|----------|--------|--------|--------|
| **P2** | Expression templates lazy eval | Optimisations auto | Élevé |
| **P2** | CUDA Graph capture pour pipelines répétitifs | -90% overhead | Élevé |
| **P3** | Buddy allocator complet | Grandes allocs | Élevé |
| **P3** | Memory pool distribué multi-GPU | Scalabilité | Très élevé |

### 4.4 Métriques de succès

```cpp
// Benchmark à implémenter pour mesurer les gains
void benchmark_improvements() {
  // 1. Latence opérations simples
  // Objectif: set_union 10K cells < 100μs (vs ~500μs actuel)
  
  // 2. Throughput batch
  // Objectif: 100 set_ops en batch < 10ms (vs ~50ms actuel)
  
  // 3. Mémoire peak
  // Objectif: réduction 30% sur pipeline AMR typique
  
  // 4. Fragmentation
  // Objectif: < 10% après 1000 ops aléatoires
}
```

---

## Annexe: Code de référence pour tests

```cpp
// Test de non-régression performance
TEST(PerformanceRegression, SetUnionLatency) {
  auto A = make_small_geometry(1000);  // 1000 cells
  auto B = make_small_geometry(1000);
  
  CsrSetAlgebraContext ctx;
  IntervalSet2DDevice out;
  
  // Warmup
  set_union_device(A, B, out, ctx);
  
  // Mesure
  auto start = std::chrono::high_resolution_clock::now();
  for (int i = 0; i < 100; ++i) {
    set_union_device(A, B, out, ctx);
  }
  auto end = std::chrono::high_resolution_clock::now();
  
  double avg_us = std::chrono::duration<double, std::micro>(end - start).count() / 100;
  
  // Assertion: doit rester sous le seuil
  EXPECT_LT(avg_us, 200.0);  // 200μs max
}
```
